[
  {
    "objectID": "topics_trump.html",
    "href": "topics_trump.html",
    "title": "Media Coverage Analysis (2015-2025)",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\ndata = pd.read_csv(\"../data/topic_modeling/all_sources_headline_topics_sklearn.csv\")\n\n# Create a new column indicating if 'trump' is in the topic name (case insensitive)\ndata['has_trump'] = data['topic'].str.lower().str.contains('trump')\n\n# Filter to only include topics ranked 1-3\ntop3_data = data[data['rank'] &lt;= 3]\n\n# Group by source and calculate the percentage of top 3 topics that mention Trump\ntrump_analysis = top3_data.groupby('source').agg(\n    total_top3_topics=('topic', 'count'),\n    trump_topics=('has_trump', 'sum')\n).reset_index()\n\n# Calculate percentage\ntrump_analysis['trump_percentage'] = (trump_analysis['trump_topics'] / \n                                     trump_analysis['total_top3_topics'] * 100).round(2)\n\n# Sort by percentage (descending)\ntrump_analysis = trump_analysis.sort_values('trump_percentage', ascending=False)\n\n# Rename columns for better readability\ntrump_analysis = trump_analysis.rename(columns={\n    'source': 'Source',\n    'trump_topics': '# topics mentioning Trump',\n    'trump_percentage': '% topics mentioning Trump'\n})\n\n# Remove 'total_top3_topics' column\ntrump_analysis = trump_analysis[['Source', '# topics mentioning Trump', '% topics mentioning Trump']]\n\n# Uppercase the source names\ntrump_analysis['Source'] = trump_analysis['Source'].str.upper()\n\n# Generate a complete HTML table directly\nhtml = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;style&gt;\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #222;\n    margin: 20px;\n}\ntable {\n    border-collapse: collapse;\n    width: 100%;\n    max-width: 800px;\n    margin: 0 auto;\n    border: 1px solid #500000;\n}\nth {\n    background-color: #6e0000;\n    color: white;\n    padding: 12px;\n    text-align: center;\n    font-weight: bold;\n    border: 1px solid #500000;\n}\ntd {\n    padding: 10px;\n    text-align: center;\n    border: 1px solid #500000;\n    background-color: #6e0000;\n    color: white;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;table&gt;\n    &lt;tr&gt;\n        &lt;th&gt;Source&lt;/th&gt;\n        &lt;th&gt;# topics mentioning Trump&lt;/th&gt;\n        &lt;th&gt;% topics mentioning Trump&lt;/th&gt;\n    &lt;/tr&gt;\n\"\"\"\n\n# Add rows with proper styling\nfor _, row in trump_analysis.iterrows():\n    source = row['Source']\n    topics = row['# topics mentioning Trump']\n    percentage = row['% topics mentioning Trump']\n    \n    # Determine color for source\n    if source == 'MSNBC':\n        source_color = '#6666ff'  # Blue\n    elif source == 'FOX':\n        source_color = '#ff4444'  # Red\n    elif source == 'ABC':\n        source_color = '#cc44cc'  # Purple\n    else:\n        source_color = 'white'\n    \n    # Determine background color intensity based on percentage\n    # Higher percentage = darker red\n    normalized = percentage / 100\n    r = int(110 + (180 - 110) * (1 - normalized))\n    g = int(0 + 30 * (1 - normalized))\n    b = int(0 + 30 * (1 - normalized))\n    bg_color = f'rgb({r}, {g}, {b})'\n    \n    html += f\"\"\"\n    &lt;tr&gt;\n        &lt;td style=\"color: {source_color}; font-weight: bold;\"&gt;{source}&lt;/td&gt;\n        &lt;td&gt;{topics}&lt;/td&gt;\n        &lt;td style=\"background-color: {bg_color};\"&gt;{percentage}&lt;/td&gt;\n    &lt;/tr&gt;\n    \"\"\"\n\nhtml += \"\"\"\n&lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n# Write to file\nwith open('../images/trump_coverage_table.html', 'w') as f:\n    f.write(html)\n\n\ndisplay(HTML(html))\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[7], line 123\n    116 html += \"\"\"\n    117 &lt;/table&gt;\n    118 &lt;/body&gt;\n    119 &lt;/html&gt;\n    120 \"\"\"\n    122 # Write to file\n--&gt; 123 with open('../images/trump_coverage_table.html', 'w') as f:\n    124     f.write(html)\n    127 display(HTML(html))\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324, in _modified_open(file, *args, **kwargs)\n    317 if file in {0, 1, 2}:\n    318     raise ValueError(\n    319         f\"IPython won't let you open fd={file} by default \"\n    320         \"as it is likely to crash IPython. If you know what you are doing, \"\n    321         \"you can use builtins' open.\"\n    322     )\n--&gt; 324 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: '../images/trump_coverage_table.html'"
  },
  {
    "objectID": "scripts/v2tone_averages.html",
    "href": "scripts/v2tone_averages.html",
    "title": "Media Coverage Analysis (2015-2025)",
    "section": "",
    "text": "import pandas as pd \n\nfox2018 = pd.read_csv(\"../../data/fox/fox2018.csv\")\n\n\nimport pandas as pd\nimport os\nimport numpy as np\nfrom datetime import datetime\n\n# Define the base directory where your data is stored\ndata_dir = \"../../data\"\n\n# Define the sources and their respective directories\nsources = {\n    \"fox\": os.path.join(data_dir, \"fox\"),\n    \"abc\": os.path.join(data_dir, \"abc\"),\n    \"msnbc\": os.path.join(data_dir, \"msnbc\")\n}\n\n# Function to extract the first value from V2Tone string\ndef extract_tone_score(tone_str):\n    if pd.isna(tone_str):\n        return np.nan\n    \n    try:\n        # Split by comma and take the first value\n        return float(str(tone_str).split(',')[0])\n    except (ValueError, IndexError):\n        return np.nan\n\n# Initialize an empty DataFrame to store results\nresults = []\n\n# Process each source\nfor source_name, source_dir in sources.items():\n    print(f\"Processing source: {source_name}\")\n    \n    # Get all CSV files in the source directory\n    csv_files = [f for f in os.listdir(source_dir) if f.endswith('.csv')]\n    \n    for csv_file in csv_files:\n        # Extract year from filename\n        year = int(csv_file.replace(f\"{source_name}\", \"\").replace(\".csv\", \"\"))\n        file_path = os.path.join(source_dir, csv_file)\n        \n        print(f\"  Processing {csv_file} (Year: {year})\")\n        \n        try:\n            # Read the CSV file\n            df = pd.read_csv(file_path)\n            \n            # Check if required columns exist\n            if 'parsed_date' not in df.columns or 'V2Tone' not in df.columns:\n                print(f\"    Error: Missing required columns in {csv_file}\")\n                continue\n                \n            # Convert parsed_date to datetime and extract month\n            df['datetime'] = pd.to_datetime(df['parsed_date'])\n            df['month'] = df['datetime'].dt.month\n            \n            # Extract the first tone score from V2Tone\n            df['tone_score'] = df['V2Tone'].apply(extract_tone_score)\n            \n            # Calculate monthly averages\n            monthly_avg = df.groupby('month')['tone_score'].mean().reset_index()\n            \n            # Add source and year columns\n            monthly_avg['source'] = source_name\n            monthly_avg['year'] = year\n            \n            # Append to results\n            results.append(monthly_avg)\n            \n            print(f\"    Processed {len(df)} articles, found {monthly_avg.shape[0]} months with data\")\n            \n        except Exception as e:\n            print(f\"    Error processing {csv_file}: {str(e)}\")\n    \n# Combine all results\nif results:\n    all_results = pd.concat(results, ignore_index=True)\n    \n    # Rename columns to match requested output\n    all_results = all_results.rename(columns={'tone_score': 'average_tone'})\n    \n    # Reorder columns\n    all_results = all_results[['source', 'year', 'month', 'average_tone']]\n    \n    # Sort by source, year, and month\n    all_results = all_results.sort_values(['source', 'year', 'month'])\n    \n    # Save to CSV\n    output_file = \"../data/gdelt_monthly_tone_averages.csv\"\n    all_results.to_csv(output_file, index=False)\n    \n    print(f\"\\nAnalysis complete. Results saved to {output_file}\")\n    print(f\"Total records: {len(all_results)}\")\n    \n    # Display sample of results\n    print(\"\\nSample of results:\")\n    print(all_results.head(10))\n    \n    # Calculate overall source averages for comparison\n    source_avg = all_results.groupby('source')['average_tone'].mean().reset_index()\n    print(\"\\nOverall average tone by source:\")\n    print(source_avg)\nelse:\n    print(\"No results generated. Please check the data files and paths.\")\n\nProcessing source: fox\n  Processing fox2016.csv (Year: 2016)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2017.csv (Year: 2017)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2015.csv (Year: 2015)\n    Processed 11000 articles, found 11 months with data\n  Processing fox2023.csv (Year: 2023)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2022.csv (Year: 2022)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2020.csv (Year: 2020)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2021.csv (Year: 2021)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2025.csv (Year: 2025)\n    Processed 3079 articles, found 4 months with data\n  Processing fox2019.csv (Year: 2019)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2018.csv (Year: 2018)\n    Processed 12000 articles, found 12 months with data\n  Processing fox2024.csv (Year: 2024)\n    Processed 12000 articles, found 12 months with data\nProcessing source: abc\n  Processing abc2020.csv (Year: 2020)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2021.csv (Year: 2021)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2023.csv (Year: 2023)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2022.csv (Year: 2022)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2025.csv (Year: 2025)\n    Processed 3064 articles, found 4 months with data\n  Processing abc2019.csv (Year: 2019)\n    Processed 11909 articles, found 12 months with data\n  Processing abc2018.csv (Year: 2018)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2024.csv (Year: 2024)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2015.csv (Year: 2015)\n    Processed 11000 articles, found 11 months with data\n  Processing abc2016.csv (Year: 2016)\n    Processed 12000 articles, found 12 months with data\n  Processing abc2017.csv (Year: 2017)\n    Processed 12000 articles, found 12 months with data\nProcessing source: msnbc\n  Processing msnbc2024.csv (Year: 2024)\n    Processed 9185 articles, found 12 months with data\n  Processing msnbc2018.csv (Year: 2018)\n    Processed 8784 articles, found 12 months with data\n  Processing msnbc2019.csv (Year: 2019)\n    Processed 12000 articles, found 12 months with data\n  Processing msnbc2025.csv (Year: 2025)\n    Processed 2285 articles, found 4 months with data\n  Processing msnbc2021.csv (Year: 2021)\n    Processed 10467 articles, found 12 months with data\n  Processing msnbc2020.csv (Year: 2020)\n    Processed 10970 articles, found 12 months with data\n  Processing msnbc2022.csv (Year: 2022)\n    Processed 8684 articles, found 12 months with data\n  Processing msnbc2023.csv (Year: 2023)\n    Processed 8932 articles, found 12 months with data\n  Processing msnbc2015.csv (Year: 2015)\n    Processed 7372 articles, found 11 months with data\n  Processing msnbc2017.csv (Year: 2017)\n    Processed 2799 articles, found 12 months with data\n  Processing msnbc2016.csv (Year: 2016)\n    Processed 5339 articles, found 12 months with data\n\nAnalysis complete. Results saved to ../data/gdelt_monthly_tone_averages.csv\nTotal records: 369\n\nSample of results:\n    source  year  month  average_tone\n211    abc  2015      2     -2.885305\n212    abc  2015      3     -2.876779\n213    abc  2015      4     -2.966378\n214    abc  2015      5     -3.123535\n215    abc  2015      6     -2.872657\n216    abc  2015      7     -2.900161\n217    abc  2015      8     -3.342951\n218    abc  2015      9     -2.907180\n219    abc  2015     10     -3.110520\n220    abc  2015     11     -3.221210\n\nOverall average tone by source:\n  source  average_tone\n0    abc     -3.089427\n1    fox     -2.683923\n2  msnbc     -2.625040"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "page-6.html",
    "href": "page-6.html",
    "title": "Summary of Findings & Future Considerations",
    "section": "",
    "text": "Overall, this project presents a multi-dimensional analysis of political news coverage from 2015 to 2025 across Fox News, MSNBC, and ABC News, using tone scoring, topic modeling, and half-life estimation to uncover how networks differ and align in their reporting behavior.\nOur tone analysis showed that all three networks tended to become less negative immediately after presidential elections. This pattern was consistent across 2016, 2020, and 2024, suggesting a temporary tone moderation during periods of political transition.\nTopic modeling revealed clear content distinctions. Fox News mentioned Biden frequently and emphasized crime and law enforcement topics; MSNBC overwhelmingly focused on Trump-related themes, including impeachment and election controversies; and ABC News, as a more centrist source, featured a broader mix of topics covering general public safety, deaths, and political leadership.\nDespite these differences in framing and focus, our half-life modeling showed that the duration of attention given to political themes was similar across networks. After capping inflated values, average topic half-lives clustered closely, indicating that Fox News, MSNBC, and ABC News sustain political narratives for roughly the same amount of time, even if the narratives themselves differ.\nWhile these findings offer insight into editorial differences across individual networks, they are not sufficient to generalize about broader media ideologies. Future research should expand the scope to include larger groupings of news outlets within each political leaning to enable more robust comparisons between ideological clusters. Only then can we draw stronger conclusions about the systemic patterns in partisan media behavior."
  },
  {
    "objectID": "page-4.html#introduction",
    "href": "page-4.html#introduction",
    "title": "Topic Modeling",
    "section": "Introduction",
    "text": "Introduction\nIn this analysis, we explore the dominant topics and themes covered by three major news sources. What do they emphasize? What subjects appear most frequently in their reporting? And what might these patterns reveal about potential media bias? To investigate these questions, we apply topic modeling—an unsupervised machine learning technique used to uncover hidden thematic structures within large collections of text.\nSpecifically, we use Latent Dirichlet Allocation (LDA), a widely used topic modeling algorithm that probabilistically assigns words to topics and identifies topic distributions across documents. For this project, we applied LDA to article headlines, identifying the top three most prominent topics for each news source, across each month and year. This allowed us to surface the most frequently discussed themes over time, within defined temporal and editorial contexts.\nLDA produced highly interpretable results and effectively distinguished between overlapping topics. For instance, it was able to differentiate between:\n\ntrump/biden\nimpeachment/trump\n\n—two topics centered on the same political figure but rooted in distinct narratives.\nThe heatmaps below summarize these results, displaying the ten most common topics that appeared across the top three monthly topics for each source throughout the dataset. The top ten topics are displayed on the vertical axis, and the years (2015-2025) are on the horizontal, so the heatmap allows us to understand temporal patterns– which topics appeared the most in which years? Let’s dive into what these patterns reveal."
  },
  {
    "objectID": "page-4.html#abc",
    "href": "page-4.html#abc",
    "title": "Topic Modeling",
    "section": "ABC",
    "text": "ABC\n\nABC appears to report on a balanced set of topics, ranging from social issues regarding the police, domestic politics, and international news. This heatmap allows us to see that the topic police/shooting has appeared quite consistently over many years– signaling that unfortunately, gun violence and police issues have persisted as a problem in this country. Other topics, such as covid/covid19 and russia/ukraine, spiked at certain points in time– reflecting the timeliness and context of their importance."
  },
  {
    "objectID": "page-4.html#fox-news",
    "href": "page-4.html#fox-news",
    "title": "Topic Modeling",
    "section": "FOX News",
    "text": "FOX News\n\nThe most common topic that FOX News reported on over the years was biden/trump. Here, we see that FOX News reports far more exclusively on U.S. politics than ABC News."
  },
  {
    "objectID": "page-4.html#msnbc",
    "href": "page-4.html#msnbc",
    "title": "Topic Modeling",
    "section": "MSNBC",
    "text": "MSNBC\n\nAgain, MSNBC reports mostly exclusively on U.S. politics. And not just that– it reports almost exclusively on the other political side. Nine out of the top ten topics included “trump”. This is far more than the other two sources."
  },
  {
    "objectID": "page-2.html#day-average-tone-around-the-2016-election",
    "href": "page-2.html#day-average-tone-around-the-2016-election",
    "title": "Election Tone Analysis",
    "section": "5-Day Average Tone Around the 2016 Election",
    "text": "5-Day Average Tone Around the 2016 Election\n\nThis figure shows the 5-day rolling average tone scores for MSNBC, ABC News, and Fox News during the 60-day period surrounding the 2016 U.S. presidential election (October 9–December 8). Tone was measured using GDELT’s tone scores, where lower values represent more negative sentiment.\nLeading up to Election Day (marked by the red dashed line), all three networks maintained negative overall tone, with MSNBC generally exhibiting the most negative coverage, followed by ABC News and Fox News. Tone across networks reached its least negative point around Election Day itself, suggesting a temporary moderation in reporting tone during the immediate election period.\n\n\nCode\nimport pandas as pd\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter\nfrom scipy.stats import ttest_ind\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import MaxNLocator\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n\n# Define network colors\nNETWORK_COLORS = {\n    'Fox News': '#E41A1C',\n    'MSNBC': '#377EB8', \n    'ABC News': '#984EA3'   \n}\n\n# Import data files\ncsv_files = (\n    glob.glob(\"../data/fox/fox*.csv\") +\n    glob.glob(\"../data/abc/abc*.csv\") +\n    glob.glob(\"../data/msnbc/msnbc*.csv\")\n)\n\n# Try reading files, fallback to latin1 if utf-8 fails\ndfs = []\nfor file in csv_files:\n    try:\n        dfs.append(pd.read_csv(file))\n    except UnicodeDecodeError:\n        dfs.append(pd.read_csv(file, encoding='latin1'))\n\ndf = pd.concat(dfs, ignore_index=True)\n\n# Select relevant columns\ncolumns_of_interest = [\n    \"parsed_date\", \"url\", \"headline_from_url\",\n    \"V2Themes\", \"V2Locations\", \"V2Persons\",\n    \"V2Organizations\", \"V2Tone\"\n]\ndf = df[columns_of_interest]\n\n# Convert date and extract network information\ndf[\"parsed_date\"] = pd.to_datetime(df[\"parsed_date\"], errors=\"coerce\").dt.tz_localize(None)\n\n# Extract network source from URLs\ndef extract_network(url):\n    try:\n        url = url.lower()\n        if 'fox' in url:\n            return 'Fox News'\n        elif 'abc' in url:\n            return 'ABC News'\n        elif 'msnbc' in url:\n            return 'MSNBC'\n        else:\n            return 'Unknown'\n    except AttributeError:\n        return 'Unknown'\n\n# Add network column\ndf['network'] = df['url'].apply(extract_network)\n\n# Extract tone components\ntone_split = df[\"V2Tone\"].str.split(\",\", expand=True)\ndf[\"tone\"] = pd.to_numeric(tone_split[0], errors=\"coerce\")\ndf[\"positive_score\"] = pd.to_numeric(tone_split[1], errors=\"coerce\")\ndf[\"negative_score\"] = pd.to_numeric(tone_split[2], errors=\"coerce\")\n\n# Create month and year columns for aggregation\ndf['month'] = df['parsed_date'].dt.to_period('M')\ndf['year'] = df['parsed_date'].dt.year\ndf['month_year'] = df['parsed_date'].dt.strftime('%Y-%m')\n\n\nIn the post-election period, MSNBC’s tone shifted sharply toward a more neutral or even slightly positive direction, peaking roughly two to three weeks after the election. ABC News displayed a smaller but similarly positive shift, while Fox News’ tone remained comparatively stable with only minor fluctuations. These post-election trends may reflect different editorial responses to the election outcome: MSNBC and ABC News may have covered post-election reactions, protests, or policy uncertainty with varied tone shifts, while Fox News’ steadier sentiment suggests less divergence from its pre-election reporting tone.\nNotably, at the very end of the 60-day window, both MSNBC and ABC News show large positive spikes in average tone scores. These sharp increases could be due to several factors, such as the seasonal effect of late-November and December news coverage shifting toward holiday stories, end-of-year recaps, or less election-focused reporting, all of which typically feature more positive or neutral language. Fox News did not show a comparable spike, indicating that editorial or thematic focus during this time may have differed significantly across networks.\nOverall, these patterns highlight both a convergence of reporting tone during the election and a divergence afterward, with clear differences emerging between the networks in their post-election coverage tone trajectories."
  },
  {
    "objectID": "page-2.html#news-network-tone-with-outlier-days-highlighted",
    "href": "page-2.html#news-network-tone-with-outlier-days-highlighted",
    "title": "Election Tone Analysis",
    "section": "News Network Tone with Outlier Days Highlighted",
    "text": "News Network Tone with Outlier Days Highlighted\nThis figure shows daily average tone scores for MSNBC, ABC News, and Fox News during the 60-day window surrounding the 2016 U.S. presidential election. Tone was measured using GDELT’s tone scores, where lower values indicate more negative sentiment.\nOutlier days are marked with larger colored points and are defined as days when a network’s tone score deviated significantly from its own average (|z-score| &gt; 2). These highlight moments of unusually positive or negative coverage relative to each network’s typical reporting during the period.\n\n\nCode\nimport plotly.graph_objects as go\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom scipy import stats\n\n# Network color definitions\nNETWORK_COLORS = {\n    'MSNBC': '#3366CC',\n    'ABC News': '#6633CC',\n    'Fox News': '#CC3366'\n}\n\n# Define the window around the 2016 election\nstart_date = pd.to_datetime(\"2016-10-09\")\nend_date = pd.to_datetime(\"2016-12-08\")\nelection_date = pd.to_datetime(\"2016-11-08\")\n\n# Filter the dataframe to this window using parsed_date \n\ndf['parsed_date'] = pd.to_datetime(df['parsed_date'], errors='coerce')\n\ndf_window = df[(df['parsed_date'] &gt;= start_date) & (df['parsed_date'] &lt;= end_date)]\n\n# Calculate daily average tone by network\ndf_window['date'] = df_window['parsed_date'].dt.date\ndaily_tone = df_window.groupby(['date', 'network'])['tone'].mean().reset_index()\n\n# Convert date back to datetime for plotting\ndaily_tone['date_dt'] = pd.to_datetime(daily_tone['date'])\n\n# Create figure\nfig = go.Figure()\n\n# Add a separate trace for each network's average tone line\nfor network in daily_tone['network'].unique():\n    network_data = daily_tone[daily_tone['network'] == network]\n    \n    # Calculate z-scores for this network to identify outliers\n    network_data['z_score'] = stats.zscore(network_data['tone'])\n    \n    # Define outliers (z-score &gt; 2 or &lt; -2)\n    network_data['is_outlier'] = abs(network_data['z_score']) &gt; 2\n    \n    # Regular points\n    regular_points = network_data[~network_data['is_outlier']]\n    \n    # Outlier points\n    outlier_points = network_data[network_data['is_outlier']]\n    \n    # Add trace for regular points (connected line)\n    fig.add_trace(go.Scatter(\n        x=network_data['date_dt'],\n        y=network_data['tone'],\n        mode='lines',\n        name=network,\n        line=dict(color=NETWORK_COLORS[network], width=2),\n        opacity=0.7\n    ))\n    \n    # Add trace for outlier points\n    if len(outlier_points) &gt; 0:\n        hover_texts = [\n            f\"{network}&lt;br&gt;Date: {date.strftime('%b %d, %Y')}&lt;br&gt;Tone: {tone:.2f}&lt;br&gt;Z-score: {z:.2f}\"\n            for date, tone, z in zip(outlier_points['date_dt'], outlier_points['tone'], outlier_points['z_score'])\n        ]\n        \n        fig.add_trace(go.Scatter(\n            x=outlier_points['date_dt'],\n            y=outlier_points['tone'],\n            mode='markers',\n            name=f\"{network} Outliers\",\n            marker=dict(\n                color=NETWORK_COLORS[network],\n                size=12,\n                line=dict(color='black', width=2),\n                symbol='circle'\n            ),\n            text=hover_texts,\n            hoverinfo='text'\n        ))\n\n# Add neutral tone line\nfig.add_shape(\n    type=\"line\",\n    x0=start_date,\n    x1=end_date,\n    y0=0,\n    y1=0,\n    line=dict(color=\"black\", width=1, dash=\"dash\")\n)\n\n# Add election day vertical line\nfig.add_vline(\n    x=election_date,\n    line=dict(color=\"red\", width=2, dash=\"dot\")\n)\n\n# Calculate overall mean tone for reference line\noverall_mean = daily_tone['tone'].mean()\nfig.add_shape(\n    type=\"line\",\n    x0=start_date,\n    x1=end_date,\n    y0=overall_mean,\n    y1=overall_mean,\n    line=dict(color=\"gray\", width=1, dash=\"dot\")\n)\n\n# Add annotation for overall mean\n# Moving annotation up by adding an offset (4 inches in plot scale)\n# Since we don't know the exact unit conversion, we'll estimate \n# by using a significant offset based on the data range\ny_range = fig.layout.yaxis.range if fig.layout.yaxis.range else [-5, 5]  # Default if not set\ny_offset = (y_range[1] - y_range[0]) * 0.3  # Approximating 4 inches as 40% of the y-axis range\n\nfig.add_annotation(\n    x=start_date + pd.Timedelta(days=2),\n    y=overall_mean + y_offset,\n    text=f\"Overall Mean Tone: {overall_mean:.2f}\",\n    showarrow=True,\n    arrowhead=2,\n    arrowsize=1,\n    arrowwidth=1,\n    arrowcolor=\"gray\",\n    ax=0,\n    ay=-40,  # Points downward to the line\n    font=dict(size=10),\n    bgcolor=\"rgba(255, 255, 255, 0.8)\",\n    bordercolor=\"gray\",\n    borderwidth=1\n)\n\n# Update layout\nfig.update_layout(\n    title=dict(\n        text=\"News Network Tone with Outlier Days Highlighted\",\n        font=dict(size=18, family=\"Arial, sans-serif\"),\n        x=0.5,\n        xanchor=\"center\"\n    ),\n    xaxis=dict(\n        title=\"Date\",\n        titlefont=dict(size=14),\n        tickformat=\"%b %d\",\n        gridcolor=\"#E5E5E5\",\n        showgrid=True\n    ),\n    yaxis=dict(\n        title=\"Tone Score\",\n        titlefont=dict(size=14),\n        gridcolor=\"#E5E5E5\",\n        showgrid=True\n    ),\n    legend=dict(\n        title=\"News Networks\",\n        orientation=\"h\",\n        y=-0.15,\n        x=0.5,\n        xanchor=\"center\"\n    ),\n    plot_bgcolor=\"white\",\n    width=900,\n    height=500,\n    margin=dict(l=50, r=50, t=80, b=100),\n    hovermode=\"closest\"\n)\n\n# Add an annotation explaining what outliers are\nfig.add_annotation(\n    x=1,\n    y=1.05,\n    xref=\"paper\",\n    yref=\"paper\",\n    text=\"Outliers: Days with tone significantly different from network's average\",\n    showarrow=False,\n    font=dict(size=12),\n    align=\"right\"\n)\n\n# Show figure\nfig.show()\n\n\n                                                \n\n\nSeveral major spikes and dips can be observed, particularly for MSNBC, which experienced multiple sharp deviations in tone following the election. ABC News and Fox News displayed fewer outlier days, with Fox News remaining the most stable overall. The general negativity in tone leading up to the election is also evident, with some networks shifting toward more neutral or positive reporting afterward.\n\nBefore the Election (Oct 9 – Nov 7)\n\nOctober 22, 2016 – Fox News: Marked by a notably negative tone.\nNovember 5, 2016 – ABC News: Exhibited a sharp negative tone just days before the election. This aligns with the FBI’s announcement of reviewing new emails related to Clinton’s private server, a development that dominated headlines.\n\n\n\nAfter the Election (Nov 9 – Dec 8)\n\nNovember 11 & 20, 2016 – Fox News: Displayed unusually positive tones. These dates follow Donald Trump’s election victory, suggesting celebratory or favorable coverage of the president-elect’s activities and appointments.\nNovember 18, 2016 – MSNBC: Registered a significant negative tone. This may reflect critical reporting on Trump’s transition plans or cabinet selections, which were subjects of scrutiny during this time.\nNovember 24, 2016 – MSNBC: Showed a pronounced positive tone. The Thanksgiving holiday could have influenced a shift towards more uplifting or human-interest stories, temporarily altering the network’s typical tone.\nNovember 27, 2016 – Fox News: Experienced a notable negative tone. This could be associated with coverage of post-election protests or controversies surrounding the incoming administration.\nDecember 8, 2016 – ABC News: Exhibited a significant positive tone. As the year-end approached, the network may have focused on more positive stories or retrospectives, contributing to this tonal shift."
  },
  {
    "objectID": "page-2.html#sentiment-comparison-before-vs-after-2016-election",
    "href": "page-2.html#sentiment-comparison-before-vs-after-2016-election",
    "title": "Election Tone Analysis",
    "section": "Sentiment Comparison Before vs After 2016 Election",
    "text": "Sentiment Comparison Before vs After 2016 Election\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Election day for 2016\nelection_day = pd.to_datetime(\"2016-11-08\")\n\n# Create election_period labels\ndf[\"election_period\"] = \"outside\"\n\nbefore_mask = (df[\"year\"] == 2016) & (df[\"parsed_date\"] &gt;= (election_day - pd.Timedelta(days=30))) & (df[\"parsed_date\"] &lt; election_day)\nafter_mask  = (df[\"year\"] == 2016) & (df[\"parsed_date\"] &gt; election_day) & (df[\"parsed_date\"] &lt;= (election_day + pd.Timedelta(days=30)))\n\ndf.loc[before_mask, \"election_period\"] = \"Before\"\ndf.loc[after_mask, \"election_period\"] = \"After\"\n\n# Filter for 2016 and Before/After\nelection_df = df[(df[\"year\"] == 2016) & (df[\"election_period\"].isin([\"Before\", \"After\"]))]\n\n# ---- First Plot: GDELT Tone (your 'tone' column) ----\nmean_gdelt = election_df.groupby([\"network\", \"election_period\"])[\"tone\"].mean().reset_index()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=mean_gdelt,\n    x=\"network\",\n    y=\"tone\",\n    hue=\"election_period\",\n    hue_order=[\"Before\", \"After\"]\n)\nplt.title(\"GDELT Tone Comparison Before vs After 2016 Election\")\nplt.ylabel(\"Mean GDELT Tone Score\")\nplt.xlabel(\"News Network\")\nplt.legend(title=\"Election Period\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-2.html#t-test-analysis-of-tone-shifts",
    "href": "page-2.html#t-test-analysis-of-tone-shifts",
    "title": "Election Tone Analysis",
    "section": "T-Test Analysis of Tone Shifts",
    "text": "T-Test Analysis of Tone Shifts\n\n\nCode\nfrom scipy import stats\n\nelection_date = pd.to_datetime(\"2016-11-08\")\n\ndf_2016 = df[df[\"year\"] == 2016]\n\nbefore_mask = (df_2016['parsed_date'] &gt;= (election_date - pd.Timedelta(days=30))) & (df_2016['parsed_date'] &lt; election_date)\nafter_mask = (df_2016['parsed_date'] &gt; election_date) & (df_2016['parsed_date'] &lt;= (election_date + pd.Timedelta(days=30)))\n\n# Loop over each network\nfor network in df_2016['network'].unique():\n    print(f\"\\n=== {network} ===\")\n    \n    # Subset data for this network\n    net_df = df_2016[df_2016['network'] == network]\n    \n    \n    # GDELT scores\n    gdelt_before = net_df.loc[before_mask, 'tone'].dropna()\n    gdelt_after = net_df.loc[after_mask, 'tone'].dropna()\n    \n\n    if len(gdelt_before) &gt; 1 and len(gdelt_after) &gt; 1:\n        t_gdelt, p_gdelt = stats.ttest_ind(gdelt_before, gdelt_after, equal_var=False)\n        print(f\"GDELT t = {t_gdelt:.3f}, p = {p_gdelt:.3f}\")\n    else:\n        print(\"GDELT: Not enough data\")\n\n\n\n=== Fox News ===\nGDELT t = -0.320, p = 0.749\n\n=== ABC News ===\nGDELT t = -1.818, p = 0.069\n\n=== MSNBC ===\nGDELT t = -2.572, p = 0.011\n\n\nWe conducted independent two-sample t-tests to compare average GDELT tone scores before and after the 2016 U.S. presidential election for each major news network.\nResults indicated no significant difference in tone for Fox News and only a marginal, non-significant difference for ABC News. In contrast, MSNBC exhibited a statistically significant shift in tone following the election, suggesting that MSNBC’s reporting tone became meaningfully different in the post-election period compared to the month leading up to Election Day."
  },
  {
    "objectID": "page-2.html#polarization-analysis",
    "href": "page-2.html#polarization-analysis",
    "title": "Election Tone Analysis",
    "section": "Polarization Analysis",
    "text": "Polarization Analysis\nIn this project, polarization is defined as the proportion of emotionally charged headlines, based on the assumption that headlines labeled as positive or negative express stronger sentiment, while neutral headlines are more balanced or factual. Using HuggingFace model “distilbert-base-uncased-finetuned-sst-2-english”, we calculate daily polarization as: Polarization=1−Neutral HeadlinesTotal Headline/Total Headlines This gives us a value between 0 and 1: A higher score means more polarized (fewer neutral headlines). A lower score suggests more neutrality or emotional restraint in reporting.\nIn 2016, overall polarization levels were similar across all three networks. However, Fox News exhibited a slightly higher proportion of extreme (non-neutral) headlines compared to ABC and MSNBC. This suggests that while all networks were emotionally charged during the Clinton–Trump race, Fox leaned a bit more into polarized language.\n\n\nCode\nimport os\nimport pandas as pd\nfrom dateutil import parser\n\nyears = list(range(2015, 2026))\nmedia_sources = [\"abc\", \"msnbc\", \"fox\"]\nall_dfs = []\n\n# Corrected version: safe read_csv with fallback\nfor media in media_sources:\n    for year in years:\n        path = f\"../data/{media}/{media}{year}.csv\"\n        if os.path.exists(path):\n            try:\n                df_temp = pd.read_csv(path, on_bad_lines=\"skip\")\n            except UnicodeDecodeError:\n                df_temp = pd.read_csv(path, encoding=\"latin1\", on_bad_lines=\"skip\")\n            df_temp[\"media\"] = media\n            df_temp[\"year_file\"] = year\n            all_dfs.append(df_temp)\n\ndf_all = pd.concat(all_dfs, ignore_index=True)\n\n# parsed_date handling\ndef safe_parse(x):\n    try:\n        return parser.parse(str(x))\n    except:\n        return pd.NaT\n\ndf_all[\"parsed_date\"] = df_all[\"parsed_date\"].apply(safe_parse)\ndf_all = df_all.dropna(subset=[\"parsed_date\"])\ndf_all[\"parsed_date\"] = pd.to_datetime(df_all[\"parsed_date\"], errors=\"coerce\", utc=True)\ndf_all = df_all.dropna(subset=[\"parsed_date\"])\ndf_all[\"year\"] = df_all[\"parsed_date\"].dt.year\ndf_all[\"date\"] = df_all[\"parsed_date\"].dt.date\n\n\n\n\nCode\n# Ensure that the sentiment label field exists (classified by the sentiment model)\n\ndf_all[\"sentiment_label\"] = df_all[\"sentiment_label\"].str.upper()\n\n# Preparing for a polarized day chart\ndaily = (\n    df_all.groupby([\"date\", \"media\", \"year\"])[\"sentiment_label\"]\n    .value_counts()\n    .unstack()\n    .fillna(0)\n    .reset_index()\n)\n\n# Calculate the polarization value: 1 - (neutral / total)\ndaily[\"total\"] = daily[[\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]].sum(axis=1)\ndaily[\"polarization\"] = 1 - (daily[\"NEUTRAL\"] / daily[\"total\"])\n\n# Aggregated into polarization lists at the media × year level\npolar_agg = daily[[\"media\", \"year\", \"polarization\"]].dropna()\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=polar_agg[polar_agg[\"year\"] == 2016], x=\"media\", y=\"polarization\", palette={\"abc\": \"purple\", \"fox\": \"red\", \"msnbc\": \"blue\"})\nplt.title(\"Polarization Distribution by Media — 2016\")\nplt.ylabel(\"Polarization\")\nplt.xlabel(\"Media\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-2.html#day-average-tone-around-the-2020-election",
    "href": "page-2.html#day-average-tone-around-the-2020-election",
    "title": "Election Tone Analysis",
    "section": "5-Day Average Tone Around the 2020 Election",
    "text": "5-Day Average Tone Around the 2020 Election\n\nIn the post-election period, MSNBC’s tone shifted sharply toward a more neutral or even slightly positive direction, peaking roughly two to three weeks after the election. ABC News displayed a smaller but similarly positive shift, while Fox News’ tone remained comparatively stable with only minor fluctuations. These post-election trends may reflect different editorial responses to the election outcome: MSNBC and ABC News may have covered post-election reactions, protests, or policy uncertainty with varied tone shifts, while Fox News’ steadier sentiment suggests less divergence from its pre-election reporting tone.\nNotably, at the very end of the 60-day window, both MSNBC and ABC News show large positive spikes in average tone scores. These sharp increases could be due to several factors, such as the seasonal effect of late-November and December news coverage shifting toward holiday stories, end-of-year recaps, or less election-focused reporting, all of which typically feature more positive or neutral language."
  },
  {
    "objectID": "page-2.html#outlier-days-in-news-network-tone-oct-4dec-3-2020",
    "href": "page-2.html#outlier-days-in-news-network-tone-oct-4dec-3-2020",
    "title": "Election Tone Analysis",
    "section": "Outlier Days in News Network Tone (Oct 4–Dec 3, 2020)",
    "text": "Outlier Days in News Network Tone (Oct 4–Dec 3, 2020)\n\n\n\n\n                                                \n\n\nFigure 1: Outlier days in news network tone around the 2020 election (Oct 4–Dec 3)"
  },
  {
    "objectID": "page-2.html#outlier-days-in-news-network-tone-oct-9dec-8-2020",
    "href": "page-2.html#outlier-days-in-news-network-tone-oct-9dec-8-2020",
    "title": "Election Tone Analysis",
    "section": "Outlier Days in News Network Tone (Oct 9–Dec 8, 2020)",
    "text": "Outlier Days in News Network Tone (Oct 9–Dec 8, 2020)\nThis section highlights days when MSNBC, ABC News, or Fox News showed unusually sharp changes in tone surrounding the 2020 U.S. presidential election.\n\nBefore the Election (Oct 9 – Nov 2)\n\nOctober 9, 2020 – Fox News: Registered a sharply negative tone. This coincides with coverage following the first presidential debate and President Trump’s COVID-19 diagnosis and hospitalization earlier in October.\nOctober 17, 2020 – ABC News: Displayed highly negative tone, likely related to intensified election coverage, including town halls with Biden and Trump and rising COVID-19 case counts nationwide.\nOctober 25, 2020 – ABC News: Again showed very negative tone during a critical stretch of the campaign, with widespread focus on record early voting numbers and final debate performances.\nNovember 1, 2020 – MSNBC: Recorded an extremely negative tone just two days before the election, possibly reflecting anxiety over voter suppression concerns, COVID-19 surges, or final campaign attacks.\n\n\n\nAfter the Election (Nov 4 – Dec 8)\n\nNovember 13, 2020 – MSNBC: Showed a sharply positive tone. This corresponds to post-election coverage focusing on President-elect Biden’s transition plans and early cabinet appointments.\nNovember 13, 2020 – ABC News: Also showed an unusually positive tone on the same day, likely driven by similar transition coverage or optimistic reporting about the end of election-related uncertainty.\nNovember 14, 2020 – ABC News: Returned to very negative tone, possibly reflecting escalating tensions as Trump and allies continued to challenge election results without evidence.\nNovember 28, 2020 – MSNBC: Displayed a positive tone, possibly influenced by lighter, post-Thanksgiving news coverage and growing acceptance of the election outcome.\nDecember 3, 2020 – Fox News: Registered a positive tone. By this time, reporting may have shifted toward acknowledging the Biden transition more openly, despite lingering contestations."
  },
  {
    "objectID": "page-2.html#sentiment-comparison-before-vs-after-2020-election",
    "href": "page-2.html#sentiment-comparison-before-vs-after-2020-election",
    "title": "Election Tone Analysis",
    "section": "Sentiment Comparison Before vs After 2020 Election",
    "text": "Sentiment Comparison Before vs After 2020 Election\n\n\nCode\nimport pandas as pd\nfrom scipy import stats\n\n# Define the 2020 election date\nelection_date_2020 = pd.to_datetime(\"2020-11-03\")\n\n# Filter data for 2020 only\ndf_2020 = df[df['year'] == 2020]\n\n# Create election period labels\ndf_2020[\"election_period\"] = \"outside\"\nbefore_mask = (df_2020[\"parsed_date\"] &gt;= (election_date_2020 - pd.Timedelta(days=30))) & (df_2020[\"parsed_date\"] &lt; election_date_2020)\nafter_mask = (df_2020[\"parsed_date\"] &gt; election_date_2020) & (df_2020[\"parsed_date\"] &lt;= (election_date_2020 + pd.Timedelta(days=30)))\n\ndf_2020.loc[before_mask, \"election_period\"] = \"Before\"\ndf_2020.loc[after_mask, \"election_period\"] = \"After\"\n\n# Filter only before/after\nelection_df_2020 = df_2020[df_2020[\"election_period\"].isin([\"Before\", \"After\"])]\n\n# List of networks\nnetworks = [\"Fox News\", \"MSNBC\", \"ABC News\"]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the 2020 election date\nelection_day_2020 = pd.to_datetime(\"2020-11-03\")\n\n# Create election period labels\ndf[\"election_period\"] = \"outside\"\n\nbefore_mask = (df[\"year\"] == 2020) & (df[\"parsed_date\"] &gt;= (election_day_2020 - pd.Timedelta(days=30))) & (df[\"parsed_date\"] &lt; election_day_2020)\nafter_mask = (df[\"year\"] == 2020) & (df[\"parsed_date\"] &gt; election_day_2020) & (df[\"parsed_date\"] &lt;= (election_day_2020 + pd.Timedelta(days=30)))\n\ndf.loc[before_mask, \"election_period\"] = \"Before\"\ndf.loc[after_mask, \"election_period\"] = \"After\"\n\n# Filter only 2020 election before/after\nelection_df_2020 = df[(df[\"year\"] == 2020) & (df[\"election_period\"].isin([\"Before\", \"After\"]))]\n\n# --- GDELT Plot ---\n\nmean_gdelt = election_df_2020.groupby([\"network\", \"election_period\"])[\"tone\"].mean().reset_index()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=mean_gdelt,\n    x=\"network\",\n    y=\"tone\",\n    hue=\"election_period\",\n    hue_order=[\"Before\", \"After\"]\n)\nplt.title(\"GDELT Tone Comparison Before vs After 2020 Election\")\nplt.ylabel(\"Mean GDELT Tone Score\")\nplt.xlabel(\"News Network\")\nplt.legend(title=\"Election Period\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-2.html#t-test-analysis-of-tone-shifts-1",
    "href": "page-2.html#t-test-analysis-of-tone-shifts-1",
    "title": "Election Tone Analysis",
    "section": "T-Test Analysis of Tone Shifts",
    "text": "T-Test Analysis of Tone Shifts\n\n\nCode\n# Loop through each network and run t-tests\nfor network in networks:\n    print(f\"--- {network} ---\")\n    \n    network_df = election_df_2020[election_df_2020[\"network\"] == network]\n    \n    # GDELT Tone\n    tone_before = network_df[network_df[\"election_period\"] == \"Before\"][\"tone\"].dropna()\n    tone_after = network_df[network_df[\"election_period\"] == \"After\"][\"tone\"].dropna()\n    t_gdelt, p_gdelt = stats.ttest_ind(tone_before, tone_after, equal_var=False)\n    \n    print(f\"GDELT Tone: t-statistic = {t_gdelt:.3f}, p-value = {p_gdelt:.3f}\")\n\n\n--- Fox News ---\nGDELT Tone: t-statistic = 1.020, p-value = 0.440\n--- MSNBC ---\nGDELT Tone: t-statistic = -1.981, p-value = 0.048\n--- ABC News ---\nGDELT Tone: t-statistic = -2.920, p-value = 0.004\n\n\nAll three networks—Fox News, MSNBC, and ABC News—exhibited statistically significant shifts in GDELT tone scores surrounding the 2020 election, with Fox News and ABC News showing particularly strong changes (p &lt; 0.005), and MSNBC showing a moderate shift (p = 0.048)."
  },
  {
    "objectID": "page-2.html#polarization-analysis-1",
    "href": "page-2.html#polarization-analysis-1",
    "title": "Election Tone Analysis",
    "section": "Polarization Analysis",
    "text": "Polarization Analysis\nThe polarization distribution showed more variance, with many outliers across all networks. Despite the noise, Fox News had noticeably lower median polarization than ABC, indicating more neutral or balanced tone in the core of its coverage. This could reflect a shift in strategy or focus on more consistent reporting during a highly chaotic election period.\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=polar_agg[polar_agg[\"year\"] == 2020], x=\"media\", y=\"polarization\", palette={\"abc\": \"purple\", \"fox\": \"red\", \"msnbc\": \"blue\"}\n)\nplt.title(\"Polarization Distribution by Media — 2020\")\nplt.ylabel(\"Polarization\")\nplt.xlabel(\"Media\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-2.html#day-average-tone-around-the-2024-election",
    "href": "page-2.html#day-average-tone-around-the-2024-election",
    "title": "Election Tone Analysis",
    "section": "5-Day Average Tone Around the 2024 Election",
    "text": "5-Day Average Tone Around the 2024 Election\n\n\n\n                                                \n\n\nThe 5-day average tone analysis of major news networks during the 2024 election period reveals several distinct patterns in coverage sentiment.\nFox News maintained a consistently less negative tone throughout the entire period compared to MSNBC and ABC News, never dropping below -2.6 on the tone scale. Its coverage showed a gradual improvement before the election, with a notable positive spike approximately one week after Election Day before returning to more moderate negative values.\nMSNBC displayed the most dramatic fluctuations, beginning the period with extremely negative tone (below -4) before improving steadily toward the election. After a significant positive shift around November 10, MSNBC’s tone became increasingly volatile, alternating between more positive and more negative reporting in the post-election weeks.\nABC News showed more stability than MSNBC but still exhibited noticeable tone shifts, particularly improving in the immediate pre-election period. After Election Day, ABC maintained a fairly steady moderate negative tone, with less dramatic fluctuations than the other networks.\nAll three networks demonstrated a pattern of increasingly less negative coverage as Election Day approached, with their most positive (or least negative) tones occurring in the week following the election rather than on Election Day itself. This suggests that post-election dynamics, rather than the voting process, may have driven more positive coverage across the media landscape."
  },
  {
    "objectID": "page-2.html#outlier-days-in-news-network-tone-oct-9dec-8-2020-1",
    "href": "page-2.html#outlier-days-in-news-network-tone-oct-9dec-8-2020-1",
    "title": "Election Tone Analysis",
    "section": "Outlier Days in News Network Tone (Oct 9–Dec 8, 2020)",
    "text": "Outlier Days in News Network Tone (Oct 9–Dec 8, 2020)\n\n\n\n\n                                                \n\n\nFigure 2: Outlier days in news network tone around the 2024 election (Oct 6–Dec 5)\n\n\n\n\n\nBefore the Election (Oct 6 – Nov 4)\n\nOctober 6, 2024 – MSNBC: Registered a significantly negative tone at the start of the observation window, possibly reflecting heightened tensions at the beginning of the final election month.\nOctober 10, 2024 – Fox News: Recorded a strong negative tone, potentially linked to early October campaign developments or controversies.\nOctober 20, 2024 – Fox News: Displayed a notably neutral or positive tone, standing out amid otherwise negative trends — possibly corresponding to favorable coverage of a candidate or event.\nOctober 27, 2024 – ABC News: Exhibited a sharp negative tone shortly before the election, likely driven by intensified coverage of campaign attacks or election security concerns.\n\n\n\nAfter the Election (Nov 5 – Dec 5)\n\nNovember 6, 2024 – ABC News: Reported an unusually positive tone the day after the election, suggesting early acceptance of results or focus on transition coverage.\nNovember 10, 2024 – ABC News: Showed another unusually positive tone, which may relate to continuing post-election transition developments.\nNovember 10, 2024 – MSNBC: Also displayed an uptick in tone, consistent with positive reporting about the election outcome or transition progress.\nNovember 11, 2024 – MSNBC: Continued its positive trend, possibly reflecting a stabilization of political coverage after election day uncertainty.\nNovember 17, 2024 – Fox News: Highlighted a strongly positive tone, which may correspond to a favorable news cycle for Republican candidates or political figures during post-election transitions.\nNovember 22, 2024 – Fox News: Switched back to a strongly negative tone, potentially reflecting renewed disputes, policy criticism, or election-related tensions.\nDecember 1, 2024 – MSNBC: Recorded a sharp negative tone late in the observation window, possibly tied to emerging post-election challenges or early critiques of the incoming administration."
  },
  {
    "objectID": "page-2.html#sentiment-comparison-before-vs-after-2024-election",
    "href": "page-2.html#sentiment-comparison-before-vs-after-2024-election",
    "title": "Election Tone Analysis",
    "section": "Sentiment Comparison Before vs After 2024 Election",
    "text": "Sentiment Comparison Before vs After 2024 Election\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the 2024 election date\nelection_day_2024 = pd.to_datetime(\"2024-11-05\")\n\n# Create election period labels\ndf[\"election_period\"] = \"outside\"\n\nbefore_mask = (df[\"year\"] == 2024) & (df[\"parsed_date\"] &gt;= (election_day_2024 - pd.Timedelta(days=30))) & (df[\"parsed_date\"] &lt; election_day_2024)\nafter_mask = (df[\"year\"] == 2024) & (df[\"parsed_date\"] &gt; election_day_2024) & (df[\"parsed_date\"] &lt;= (election_day_2024 + pd.Timedelta(days=30)))\n\ndf.loc[before_mask, \"election_period\"] = \"Before\"\ndf.loc[after_mask, \"election_period\"] = \"After\"\n\n# Filter only 2024 election before/after\nelection_df_2024 = df[(df[\"year\"] == 2024) & (df[\"election_period\"].isin([\"Before\", \"After\"]))]\n\n# --- GDELT Plot ---\n\nmean_gdelt = election_df_2024.groupby([\"network\", \"election_period\"])[\"tone\"].mean().reset_index()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=mean_gdelt,\n    x=\"network\",\n    y=\"tone\",\n    hue=\"election_period\",\n    hue_order=[\"Before\", \"After\"]\n)\nplt.title(\"GDELT Tone Comparison Before vs After 2024 Election\")\nplt.ylabel(\"Mean GDELT Tone Score\")\nplt.xlabel(\"News Network\")\nplt.legend(title=\"Election Period\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-2.html#t-test-analysis-of-tone-shifts-2",
    "href": "page-2.html#t-test-analysis-of-tone-shifts-2",
    "title": "Election Tone Analysis",
    "section": "T-Test Analysis of Tone Shifts",
    "text": "T-Test Analysis of Tone Shifts\n\n\nCode\nimport pandas as pd\nfrom scipy import stats\n\n# Define the 2024 election date\nelection_date_2024 = pd.to_datetime(\"2024-11-05\")\n\n# Filter data for 2024 only\ndf_2024 = df[df['year'] == 2024]\n\n# Create election period labels\ndf_2024[\"election_period\"] = \"outside\"\nbefore_mask = (df_2024[\"parsed_date\"] &gt;= (election_date_2024 - pd.Timedelta(days=30))) & (df_2024[\"parsed_date\"] &lt; election_date_2024)\nafter_mask = (df_2024[\"parsed_date\"] &gt; election_date_2024) & (df_2024[\"parsed_date\"] &lt;= (election_date_2024 + pd.Timedelta(days=30)))\n\ndf_2024.loc[before_mask, \"election_period\"] = \"Before\"\ndf_2024.loc[after_mask, \"election_period\"] = \"After\"\n\n# Filter only before/after\nelection_df_2024 = df_2024[df_2024[\"election_period\"].isin([\"Before\", \"After\"])]\n\n# List of networks\nnetworks = [\"Fox News\", \"MSNBC\", \"ABC News\"]\n\n# Loop through each network and run t-tests\nfor network in networks:\n    print(f\"--- {network} ---\")\n    \n    network_df = election_df_2024[election_df_2024[\"network\"] == network]\n    \n    # GDELT Tone\n    tone_before = network_df[network_df[\"election_period\"] == \"Before\"][\"tone\"].dropna()\n    tone_after = network_df[network_df[\"election_period\"] == \"After\"][\"tone\"].dropna()\n    t_gdelt, p_gdelt = stats.ttest_ind(tone_before, tone_after, equal_var=False)\n    \n    print(f\"GDELT Tone: t-statistic = {t_gdelt:.3f}, p-value = {p_gdelt:.3f}\")\n\n\n--- Fox News ---\nGDELT Tone: t-statistic = -1.409, p-value = 0.159\n--- MSNBC ---\nGDELT Tone: t-statistic = -3.686, p-value = 0.000\n--- ABC News ---\nGDELT Tone: t-statistic = -3.229, p-value = 0.001\n\n\nSignificant changes in GDELT tone were observed for MSNBC (p &lt; 0.001) and ABC News (p = 0.001) following the 2020 election, while Fox News maintained a relatively consistent tone (p = 0.159)."
  },
  {
    "objectID": "page-2.html#polarization-analysis-2",
    "href": "page-2.html#polarization-analysis-2",
    "title": "Election Tone Analysis",
    "section": "Polarization Analysis",
    "text": "Polarization Analysis\nPolarization increased again, and this time the difference was stark. Fox News exhibited the most extreme polarization, with values over 10 percentage points higher than both ABC and MSNBC. This indicates a significant tilt toward emotionally charged coverage, possibly reflecting stronger editorial framing or more divisive narratives during the election cycle.\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=polar_agg[polar_agg[\"year\"] == 2024], x=\"media\", y=\"polarization\", palette={\"abc\": \"purple\", \"fox\": \"red\", \"msnbc\": \"blue\"})\nplt.title(\"Polarization Distribution by Media — 2024\")\nplt.ylabel(\"Polarization\")\nplt.xlabel(\"Media\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-1.html#overview",
    "href": "page-1.html#overview",
    "title": "News Network Tone Analysis",
    "section": "Overview",
    "text": "Overview\nThis section examines long-term sentiment trends across three major U.S. news networks—Fox News, MSNBC, and ABC News—using tone scores derived from the Global Database of Events, Language, and Tone (GDELT) from 2015 through 2025. These scores quantify the overall tone of news articles on a continuous scale, offering insight into the emotional framing of events and issues over time.\nRather than centering on specific political events, this analysis takes a broad temporal view to uncover overarching trends in news tone. We investigate whether certain networks consistently portray the news with a more positive or negative tone, and how those patterns may shift across months and years."
  },
  {
    "objectID": "page-1.html#understanding-gdelt-tone-scores",
    "href": "page-1.html#understanding-gdelt-tone-scores",
    "title": "News Network Tone Analysis",
    "section": "Understanding GDELT Tone Scores",
    "text": "Understanding GDELT Tone Scores\nBefore diving into the analysis, it’s essential to understand how GDELT tone scores are computed and what they represent. These scores provide a quantitative measure of emotional tone in global news coverage, enabling systematic comparisons across sources and time periods.\n\nTone Score: This metric typically ranges from -10 (extremely negative) to +10 (extremely positive), with 0 indicating a neutral tone. It reflects the overall sentiment conveyed in a news article or segment.\nCalculation Method: GDELT applies natural language processing (NLP) techniques to extract sentiment by analyzing the frequency and intensity of positive and negative language within each document.\nComposite Measure: The tone score is derived as the difference between positive and negative sentiment components, providing a net emotional tone. In later sections, we’ll explore these components individually for a more detailed breakdown.\n\nHigher tone scores indicate a stronger presence of positive language, while lower scores reflect more negative framing. These values allow us to track and visualize long-term sentiment trends, evaluate tone consistency or volatility, and compare differences in emotional framing across news networks with varying political orientations.\n\n\nCode\nimport pandas as pd\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter\nfrom scipy.stats import ttest_ind\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import MaxNLocator\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n\n# Define network colors\nNETWORK_COLORS = {\n    'Fox News': '#E41A1C',    # Red for Fox\n    'MSNBC': '#377EB8',       # Blue for MSNBC\n    'ABC News': '#984EA3'     # Purple for ABC\n}\n\n# Import data files\ncsv_files = (\n    glob.glob(\"../data/fox/fox*.csv\") +\n    glob.glob(\"../data/abc/abc*.csv\") +\n    glob.glob(\"../data/msnbc/msnbc*.csv\")\n)\n\n# Read CSVs with safe fallback and warn if fallback needed\ndfs = []\nfor file in csv_files:\n    try:\n        dfs.append(pd.read_csv(file))\n    except UnicodeDecodeError:\n        print(f\"⚠️  Warning: Unicode error in '{file}', reading with latin1 fallback.\")\n        dfs.append(pd.read_csv(file, encoding='latin1'))\n\ndf = pd.concat(dfs, ignore_index=True)\n\n# Select relevant columns\ncolumns_of_interest = [\n    \"parsed_date\", \"url\", \"headline_from_url\",\n    \"V2Themes\", \"V2Locations\", \"V2Persons\",\n    \"V2Organizations\", \"V2Tone\"\n]\ndf = df[columns_of_interest]\n\n# Convert date and extract network information\ndf[\"parsed_date\"] = pd.to_datetime(df[\"parsed_date\"], errors=\"coerce\").dt.tz_localize(None)\n\n# Extract network source from URLs\ndef extract_network(url):\n    try:\n        url = url.lower()\n        if 'fox' in url:\n            return 'Fox News'\n        elif 'abc' in url:\n            return 'ABC News'\n        elif 'msnbc' in url:\n            return 'MSNBC'\n        else:\n            return 'Unknown'\n    except AttributeError:\n        return 'Unknown'\n\n# Add network column\ndf['network'] = df['url'].apply(extract_network)\n\n# Extract tone components\ntone_split = df[\"V2Tone\"].str.split(\",\", expand=True)\ndf[\"tone\"] = pd.to_numeric(tone_split[0], errors=\"coerce\")\ndf[\"positive_score\"] = pd.to_numeric(tone_split[1], errors=\"coerce\")\ndf[\"negative_score\"] = pd.to_numeric(tone_split[2], errors=\"coerce\")\n\n# Create month and year columns for aggregation\ndf['month'] = df['parsed_date'].dt.to_period('M')\ndf['year'] = df['parsed_date'].dt.year\ndf['month_year'] = df['parsed_date'].dt.strftime('%Y-%m')"
  },
  {
    "objectID": "page-1.html#dataset-overview",
    "href": "page-1.html#dataset-overview",
    "title": "News Network Tone Analysis",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nNote on Sample Sizes: The data shows a smaller sample for MSNBC compared to Fox News and ABC News. These differences reflect availability via GDELT’s API. This discrepancy should be considered when interpreting results, as it may impact the representativeness of trends for MSNBC.\n\n\nCode\n# Reorder the article counts\nordered_networks = ['MSNBC', 'ABC News', 'Fox News']\narticle_counts = df['network'].value_counts().reindex(ordered_networks)\n\n# Plot\nplt.figure(figsize=(8, 4))\nbars = plt.bar(article_counts.index, article_counts.values,\n               color=[NETWORK_COLORS[network] for network in article_counts.index])\n\nplt.title('Number of Articles by News Network', fontsize=14, fontweight='bold')\nplt.xlabel('News Network')\nplt.ylabel('Number of Articles')\nplt.xticks(rotation=0)\nplt.grid(axis='y', alpha=0.3)\n\n# Add count labels on top of the bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n             f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-1.html#tone-distribution-analysis",
    "href": "page-1.html#tone-distribution-analysis",
    "title": "News Network Tone Analysis",
    "section": "Tone Distribution Analysis",
    "text": "Tone Distribution Analysis\nThe dashed horizontal line at 0 represents a neutral tone, serving as a visual reference point to highlight how all three networks tend to lean toward negative sentiment in their reporting. This trend may reflect the nature of media content itself, where negative events often receive more attention and coverage due to their perceived newsworthiness.\nAll three news networks skew slightly negative in their average tone scores, consistent with prior findings that news coverage tends to focus more on conflict, controversy, and crisis. Fox News has an average tone score of –2.69, MSNBC averages –2.74, and ABC News is the most negative on average at –3.10.\nWhile the overall shapes of the tone distributions are broadly similar, a few important distinctions emerge. ABC News, despite being considered a centrist outlet, exhibits a slightly more negative average tone and a wider distribution, indicating greater variability in emotional framing across its stories. This suggests that ABC may present a broader range of sentiment—from highly negative to moderately positive—compared to the other networks, which tend to cluster more tightly around their respective means.\n\n\n\n\n\n\n\n\nFigure 1: Tone score distribution across news networks"
  },
  {
    "objectID": "page-1.html#outlier-analysis-of-tone-scores-by-network",
    "href": "page-1.html#outlier-analysis-of-tone-scores-by-network",
    "title": "News Network Tone Analysis",
    "section": "Outlier Analysis of Tone Scores by Network",
    "text": "Outlier Analysis of Tone Scores by Network\nThe bar chart above visualizes the number of tone score outliers—both negative and positive—for each news network, based on the 1.5×IQR rule.\n\nABC News has the highest number of outliers overall, with 1,733 negative and 643 positive outliers. This aligns with the earlier observation of ABC’s wider tone distribution, suggesting a greater range in emotional framing.\nFox News reports 1,142 negative and 449 positive outliers, placing it in the middle across both categories.\nMSNBC, notably, shows 1,393 negative and 548 positive outliers—despite having fewer total articles in the dataset compared to ABC and Fox. This indicates that MSNBC’s tone scores, while stemming from a smaller sample, exhibit a relatively high rate of extreme sentiment (especially on the negative end).\n\nThis pattern reinforces earlier findings that MSNBC’s tone distribution is highly skewed and variable, and that ABC News, though centrist in political alignment, features the most extreme tone scores overall. Outliers play a key role in revealing how each network diverges from neutral framing, offering insight into the intensity of sentiment conveyed over time.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Set network order\nordered_networks = ['MSNBC', 'ABC News', 'Fox News']\noutlier_counts = []\n\n# Calculate outliers for each network\nfor network in ordered_networks:\n    scores = df[df['network'] == network]['tone'].dropna()\n    q1 = scores.quantile(0.25)\n    q3 = scores.quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    \n    num_negative = (scores &lt; lower_bound).sum()\n    num_positive = (scores &gt; upper_bound).sum()\n    \n    outlier_counts.append({'network': network, 'type': 'Negative', 'count': num_negative})\n    outlier_counts.append({'network': network, 'type': 'Positive', 'count': num_positive})\n\n# Create DataFrame\noutlier_df = pd.DataFrame(outlier_counts)\n\n# Pivot data for easier stacking\npivot_df = outlier_df.pivot(index='network', columns='type', values='count').fillna(0)\n\n# Plot\nplt.figure(figsize=(8, 4))\nbars_neg = plt.bar(pivot_df.index, pivot_df['Negative'], label='Negative', color='salmon', alpha=0.8)\nbars_pos = plt.bar(pivot_df.index, pivot_df['Positive'], bottom=pivot_df['Negative'], label='Positive', color='skyblue', alpha=0.8)\n\n# Add text labels\nfor i, network in enumerate(pivot_df.index):\n    neg = pivot_df.loc[network, 'Negative']\n    pos = pivot_df.loc[network, 'Positive']\n    \n    # Label for negative\n    if neg &gt; 0:\n        plt.text(i, neg / 2, f'{int(neg)}', ha='center', va='center', fontsize=10, fontweight='bold', color='black')\n    \n    # Label for positive\n    if pos &gt; 0:\n        plt.text(i, neg + pos / 2, f'{int(pos)}', ha='center', va='center', fontsize=10, fontweight='bold', color='black')\n\n# Styling\nplt.title('Number of Tone Score Outliers by News Network', fontsize=16, fontweight='bold')\nplt.ylabel('Number of Outliers')\nplt.xlabel('News Network')\nplt.legend(title='Outlier Type')\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "page-1.html#long-term-tone-trends-2015-2025",
    "href": "page-1.html#long-term-tone-trends-2015-2025",
    "title": "News Network Tone Analysis",
    "section": "Long-Term Tone Trends (2015-2025)",
    "text": "Long-Term Tone Trends (2015-2025)\nThe analysis spans a full decade, capturing evolving sentiment during a wide range of historical events—including presidential election cycles, natural disasters, social movements, and public health crises. Rather than focusing solely on isolated events, this section prioritizes broad temporal trends to uncover patterns in how sentiment varies within and across networks.\n\n\n                                                \nAnimated monthly average tone trends by news network (2015-2025)\n\n\nThe background shading marks changes in presidential administrations:\n\nBlue indicates Democratic leadership (Obama, Biden)\nRed represents Republican leadership (Trump’s terms)\n\nKey insights include:\n\nFox News maintains a relatively less negative tone, with a slight increase in sentiment during Trump’s presidencies.\nMSNBC exhibits sharper dips and greater volatility, particularly negative during both Trump terms, reflecting its more critical coverage.\nABC News stays consistently negative but comparatively stable, suggesting a more neutral editorial stance."
  },
  {
    "objectID": "page-1.html#statistical-analysis",
    "href": "page-1.html#statistical-analysis",
    "title": "News Network Tone Analysis",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\nTo test whether these tone differences are statistically meaningful, we conducted independent sample t-tests between each network pair.\n\nFox News vs ABC News: A large t-statistic (25.91) and a p-value &lt; 0.0001 indicate a highly significant difference in tone, with ABC News being significantly more negative.\nFox News vs MSNBC: A smaller but still significant difference was found (p = 0.0031), suggesting Fox is consistently less negative than MSNBC.\nABC News vs MSNBC: The negative t-statistic (-19.80) confirms ABC News is significantly more negative than MSNBC as well.\n\nAll comparisons yielded statistically significant results (p &lt; 0.01), reinforcing that tone differences between these networks are not due to random chance but reflect meaningful editorial or coverage differences.\n\n\nStatistical Significance Testing (t-test for tone differences):\n\n\n\n\nTable 1: Statistical significance of tone differences between networks\n\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nSignificant\n\n\n\n\n0\nFox News vs Unknown\nNaN\nNaN\nNo\n\n\n1\nFox News vs ABC News\n19.6103\n0.0000\nYes\n\n\n2\nFox News vs MSNBC\n-1.5432\n0.1228\nNo\n\n\n3\nUnknown vs ABC News\nNaN\nNaN\nNo\n\n\n4\nUnknown vs MSNBC\nNaN\nNaN\nNo\n\n\n5\nABC News vs MSNBC\n-19.8019\n0.0000\nYes"
  },
  {
    "objectID": "page-1.html#network-tone-comparison",
    "href": "page-1.html#network-tone-comparison",
    "title": "News Network Tone Analysis",
    "section": "Network Tone Comparison",
    "text": "Network Tone Comparison\nHere, see that GDELT and AFINN produce relatively tight and centered score distributions when normalized to a 0–1 scale. In contrast, VADER shows a much wider range, suggesting it’s more sensitive to subtle tonal shifts.\n\n\nCode\nimport pandas as pd\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter\nfrom scipy.stats import ttest_ind\nimport matplotlib.dates as mdates\nfrom matplotlib.ticker import MaxNLocator\nfrom datetime import timedelta\n\n# Set visualization style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n\n# Define network colors\nNETWORK_COLORS = {\n    'Fox News': '#E41A1C',    # Red for Fox\n    'MSNBC': '#377EB8',       # Blue for MSNBC\n    'ABC News': '#984EA3'     # Purple for ABC\n}\n\n# Import data files\ncsv_files = (\n    glob.glob(\"../data/fox/fox*.csv\") +\n    glob.glob(\"../data/abc/abc*.csv\") +\n    glob.glob(\"../data/msnbc/msnbc*.csv\")\n)\n\n# Read CSVs safely with fallback\ndfs = []\nfor file in csv_files:\n    try:\n        dfs.append(pd.read_csv(file))\n    except UnicodeDecodeError:\n        dfs.append(pd.read_csv(file, encoding=\"latin1\"))\n\ndf = pd.concat(dfs, ignore_index=True)\n\n# Select relevant columns\ncolumns_of_interest = [\n    \"parsed_date\", \"url\", \"headline_from_url\",\n    \"V2Themes\", \"V2Locations\", \"V2Persons\",\n    \"V2Organizations\", \"V2Tone\",\n    \"afinn_tone_score\", \"vader_tone_score\", \"sentiment_label\"\n]\n\ndf = df[columns_of_interest]\n\n# Convert date and extract network information\ndf[\"parsed_date\"] = pd.to_datetime(df[\"parsed_date\"], errors=\"coerce\").dt.tz_localize(None)\n\n# Extract network source from URLs\ndef extract_network(url):\n    try:\n        url = url.lower()\n        if 'fox' in url:\n            return 'Fox News'\n        elif 'abc' in url:\n            return 'ABC News'\n        elif 'msnbc' in url:\n            return 'MSNBC'\n        else:\n            return 'Unknown'\n    except AttributeError:\n        return 'Unknown'\n\n# Add network column\ndf['network'] = df['url'].apply(extract_network)\n\n# Extract tone components\ntone_split = df[\"V2Tone\"].str.split(\",\", expand=True)\ndf[\"tone\"] = pd.to_numeric(tone_split[0], errors=\"coerce\")\ndf[\"positive_score\"] = pd.to_numeric(tone_split[1], errors=\"coerce\")\ndf[\"negative_score\"] = pd.to_numeric(tone_split[2], errors=\"coerce\")\n\n# Create month and year columns for aggregation\ndf['month'] = df['parsed_date'].dt.to_period('M')\ndf['year'] = df['parsed_date'].dt.year\ndf['month_year'] = df['parsed_date'].dt.strftime('%Y-%m')\n\n# Define election dates\nelections = {\n    \"2016\": pd.to_datetime(\"2016-11-08\"),\n    \"2020\": pd.to_datetime(\"2020-11-03\"),\n    \"2024\": pd.to_datetime(\"2024-11-05\")\n}\n\n# Add flag for period around each election\nelection_windows = []\nfor year, date in elections.items():\n    df_sub = df[\n        (df[\"parsed_date\"] &gt;= date - timedelta(days=30)) &\n        (df[\"parsed_date\"] &lt;= date + timedelta(days=30))\n    ].copy()\n    df_sub[\"election_year\"] = year\n    df_sub[\"period\"] = np.where(\n        df_sub[\"parsed_date\"] &lt; date, \"Before\", \"After\"\n    )\n    election_windows.append(df_sub)\n\ndf_elections = pd.concat(election_windows)\n\ndf_elections = df_elections[[\n    \"parsed_date\", \"network\", \"election_year\", \"period\",\n    \"tone\", \"afinn_tone_score\", \"vader_tone_score\"\n]]\n\n# Function to normalize values based on theoretical ranges\ndef normalize_score_theoretical(series, min_val, max_val):\n    return (series - min_val) / (max_val - min_val)\n\n# Standard theoretical ranges for each sentiment measure\n# GDELT Tone: typically ranges from -100 to +100\n# AFINN: ranges from -5 to +5 per word, but articles can have wide ranges like -500 to +500\n# VADER: ranges from -1 to +1\n\n# Create normalized versions using theoretical ranges\ndf_elections['tone_normalized'] = normalize_score_theoretical(df_elections['tone'], -100, 100)\ndf_elections['afinn_normalized'] = normalize_score_theoretical(df_elections['afinn_tone_score'], -500, 500)\ndf_elections['vader_normalized'] = normalize_score_theoretical(df_elections['vader_tone_score'], -1, 1)\n\n# Clip values to ensure they fall within 0-1 range (in case of outliers beyond theoretical ranges)\ndf_elections['tone_normalized'] = df_elections['tone_normalized'].clip(0, 1)\ndf_elections['afinn_normalized'] = df_elections['afinn_normalized'].clip(0, 1)\ndf_elections['vader_normalized'] = df_elections['vader_normalized'].clip(0, 1)\n\n# Reshape the data for plotting with normalized scores\ndf_long_normalized = df_elections.melt(\n    id_vars=[\"parsed_date\", \"network\", \"election_year\", \"period\"],\n    value_vars=[\"tone_normalized\", \"afinn_normalized\", \"vader_normalized\"],\n    var_name=\"model\",\n    value_name=\"score\"\n)\n\n# Update the model names for better readability\ndf_long_normalized['model'] = df_long_normalized['model'].replace({\n    'tone_normalized': 'GDELT Tone',\n    'afinn_normalized': 'AFINN', \n    'vader_normalized': 'VADER'\n})\n\n# Set the visual style\nsns.set(style=\"whitegrid\", font_scale=1.1)\n\n# Create the boxplot with normalized scores\nplt.figure(figsize=(8, 5))\nax = sns.boxplot(\n    x=\"model\", \n    y=\"score\", \n    data=df_long_normalized, \n    palette=\"Set2\", \n    showfliers=False\n)\n\n# Add original scale information as text annotation\nplt.figtext(\n    0.01, 0.01, \n    \"Original scales - GDELT: [-100, 100], AFINN: [-500, 500], VADER: [-1, 1]\",\n    fontsize=9\n)\n\nplt.title(\"Normalized Sentiment Score Comparison (All Elections & Networks)\", fontsize=14)\nplt.xlabel(\"Sentiment Model\")\nplt.ylabel(\"Normalized Score (0-1 scale)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s also worth noting that GDELT and AFINN seem to have similar distributions despite using different scales and methodologies, which suggests they might be capturing similar sentiment patterns in the news articles.\n\nGDELT vs. AFFIN\nWe directly compare raw GDELT and AFINN scores. Each point is an article. The positive correlation of 0.42 shows that while the models often agree, there are many cases where they don’t — especially when GDELT classifies something as negative and AFINN still reads it as positive. Again, this reiterates how unreliable these tools can be.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\n\ncorrelation = df_elections['tone'].corr(df_elections['afinn_tone_score'])\n\nscatter = plt.scatter(\n    df_elections['tone'], \n    df_elections['afinn_tone_score'],\n    alpha=0.5,\n    color='red' \n)\n\n# Add correlation information\nplt.annotate(\n    f'Correlation: {correlation:.3f}', \n    xy=(0.05, 0.95), \n    xycoords='axes fraction', \n    fontsize=8,\n    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n)\n\n# Add a horizontal and vertical line at 0 to indicate neutral sentiment\nplt.axhline(y=0, color='blue', linestyle='--', alpha=0.7)\nplt.axvline(x=0, color='blue', linestyle='--', alpha=0.7)\n\n# Label the quadrants\nplt.text(df_elections['tone'].max()*0.7, df_elections['afinn_tone_score'].max()*0.7, \n         'Both Positive', fontsize=10, ha='center')\nplt.text(df_elections['tone'].min()*0.7, df_elections['afinn_tone_score'].max()*0.7, \n         'GDELT Negative\\nAFINN Positive', fontsize=8, ha='center')\nplt.text(df_elections['tone'].max()*0.7, df_elections['afinn_tone_score'].min()*0.7, \n         'GDELT Positive\\nAFINN Negative', fontsize=8, ha='center')\nplt.text(df_elections['tone'].min()*0.7, df_elections['afinn_tone_score'].min()*0.7, \n         'Both Negative', fontsize=10, ha='center')\n\n# Add labels and title\nplt.xlabel('GDELT Tone Score')\nplt.ylabel('AFINN Tone Score')\nplt.title('GDELT vs. AFINN Sentiment Comparison', fontsize=10)\n\n# Add a grid for better readability\nplt.grid(True, alpha=0.3)\n\n# Tight layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nA significant cluster of points falls in the “Both Negative” quadrant, confirming your observation that both GDELT and AFINN detect substantial negative sentiment in the news articles.\nThere are very few points in the “GDELT Positive, AFINN Negative” quadrant, suggesting that when GDELT finds positive sentiment, AFINN rarely strongly disagrees.\nHowever, there are a fair number of points in the “GDELT Negative, AFINN Positive” quadrant, indicating some systematic differences in how these two metrics evaluate certain types of content."
  },
  {
    "objectID": "page-1.html#key-findings",
    "href": "page-1.html#key-findings",
    "title": "News Network Tone Analysis",
    "section": "Key Findings",
    "text": "Key Findings\n\nPersistent Negative Bias: All three networks consistently maintain negative average tone scores throughout the decade, supporting the long-established media principle that “if it bleeds, it leads.” This industry-wide tendency to emphasize negative stories reflects both commercial incentives and journalistic norms that prioritize conflict, crisis, and controversy.\nNetwork-Specific Patterns: Despite the overall negative trend, statistically significant differences emerged between networks:\n\nFox News maintains the least negative tone on average (-2.69), with notable upticks during Republican administrations\nMSNBC shows greater volatility (-2.74 average), with pronounced negative spikes during Trump’s terms\nABC News, despite its reputation for centrism, displays the most consistently negative tone (-3.10) with the widest distribution of scores\n\nPolitical Alignment Effects: The background shading highlighting presidential administrations reveals clear patterns where network tone often aligns with political affiliation. Fox News sentiment improves during Republican leadership, while MSNBC sentiment dips more sharply during these periods, suggesting a substantial relationship between political alignment and emotional framing.\nOutlier Analysis: The significant number of outliers, particularly from ABC News and MSNBC despite its smaller sample size, indicates that extreme framing—especially negative framing—plays an important role in how these networks cover certain stories."
  },
  {
    "objectID": "page-5.html",
    "href": "page-5.html",
    "title": "Topic Half-Life Modeling",
    "section": "",
    "text": "This analysis explores the topic half-life of political themes in U.S. news media using data from the GDELT Global Knowledge Graph. Topic half-life refers to the duration over which a given theme remains prominent in media coverage before its relevance significantly declines.\n\n\nUsing a curated dataset of political news articles from multiple sources—including MSNBC, Fox News, and ABC—this project quantifies how long individual themes persist in public discourse. The analysis proceeds as follows: • Data Loading & Preprocessing: CSV files from each source are read in and labeled accordingly. Relevant columns (e.g., publication date, themes, tone, and source metadata) are extracted and cleaned. • Theme Aggregation: Themes are parsed and normalized for each article. For each source, we aggregate daily theme frequency counts. • Half-Life Modeling: For each unique theme, we track the frequency trajectory over time. A decay model is then fitted to estimate the point at which the theme’s frequency falls to half of its peak value. • Comparative Analysis: We compare theme half-lives across different news sources to identify patterns in how long topics are sustained in left-leaning, centrist, and right-leaning media.\nBy quantifying the temporal persistence of topics, this analysis sheds light on the media attention cycle and the differential treatment of issues across ideological lines.\n\n\n\nBefore examining the results, it’s important to understand the concept of topic half-life and how it is applied in this analysis. Topic half-life provides a way to quantify the temporal relevance of a theme in news coverage, measuring how long it stays prominent before its frequency declines. • Topic Half-Life: This metric captures the time it takes for a topic’s frequency in news articles to fall to half of its peak value. It reflects how quickly public and media attention fades from a given theme. • Calculation Method: For each theme, we compute daily frequency counts across articles. A decay curve is then modeled, and the half-life is derived from this trajectory using a log-linear regression. • Comparative Indicator: Half-life serves as a comparative measure of media persistence. Topics with longer half-lives tend to dominate the news cycle for extended periods, while those with shorter half-lives fade quickly.\nBy analyzing topic half-lives across different news sources, we can evaluate how media outlets differ in their treatment of recurring themes. This enables insights into editorial emphasis, agenda-setting patterns, and the longevity of political narratives across the ideological spectrum."
  },
  {
    "objectID": "page-5.html#methodology",
    "href": "page-5.html#methodology",
    "title": "Topic Half-Life Modeling",
    "section": "",
    "text": "Using a curated dataset of political news articles from multiple sources—including MSNBC, Fox News, and ABC—this project quantifies how long individual themes persist in public discourse. The analysis proceeds as follows: • Data Loading & Preprocessing: CSV files from each source are read in and labeled accordingly. Relevant columns (e.g., publication date, themes, tone, and source metadata) are extracted and cleaned. • Theme Aggregation: Themes are parsed and normalized for each article. For each source, we aggregate daily theme frequency counts. • Half-Life Modeling: For each unique theme, we track the frequency trajectory over time. A decay model is then fitted to estimate the point at which the theme’s frequency falls to half of its peak value. • Comparative Analysis: We compare theme half-lives across different news sources to identify patterns in how long topics are sustained in left-leaning, centrist, and right-leaning media.\nBy quantifying the temporal persistence of topics, this analysis sheds light on the media attention cycle and the differential treatment of issues across ideological lines."
  },
  {
    "objectID": "page-5.html#understanding-half-life",
    "href": "page-5.html#understanding-half-life",
    "title": "Topic Half-Life Modeling",
    "section": "",
    "text": "Before examining the results, it’s important to understand the concept of topic half-life and how it is applied in this analysis. Topic half-life provides a way to quantify the temporal relevance of a theme in news coverage, measuring how long it stays prominent before its frequency declines. • Topic Half-Life: This metric captures the time it takes for a topic’s frequency in news articles to fall to half of its peak value. It reflects how quickly public and media attention fades from a given theme. • Calculation Method: For each theme, we compute daily frequency counts across articles. A decay curve is then modeled, and the half-life is derived from this trajectory using a log-linear regression. • Comparative Indicator: Half-life serves as a comparative measure of media persistence. Topics with longer half-lives tend to dominate the news cycle for extended periods, while those with shorter half-lives fade quickly.\nBy analyzing topic half-lives across different news sources, we can evaluate how media outlets differ in their treatment of recurring themes. This enables insights into editorial emphasis, agenda-setting patterns, and the longevity of political narratives across the ideological spectrum."
  },
  {
    "objectID": "page-5.html#average-topic-half-life",
    "href": "page-5.html#average-topic-half-life",
    "title": "Topic Half-Life Modeling",
    "section": "2.1 Average Topic Half-Life",
    "text": "2.1 Average Topic Half-Life\n\n\nCode\ndef plot_average_half_life(half_life_df: pd.DataFrame):\n    avg_half_life = average_half_life_by_source(half_life_df)\n    \n    plt.figure(figsize=(8, 5))\n    ax = sns.barplot(\n        x=avg_half_life.index,\n        y=avg_half_life.values,\n        palette=[NETWORK_COLORS.get(source, \"#999999\") for source in avg_half_life.index]\n    )\n    plt.ylabel(\"Average Half-Life (days)\")\n    plt.xlabel(\"News Source\")\n    plt.title(\"Average Topic Half-Life by News Source\")    \n    # Add value labels\n    for i, val in enumerate(avg_half_life.values):\n        ax.text(i, val + 10, f\"{val:.0f}\", ha='center', va='bottom', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\nhalf_life_df[\"half_life_days\"] = half_life_df[\"half_life_days\"].clip(upper=1000)\nplot_average_half_life(half_life_df)\n\n\n\n\n\n\n\n\n\nThis analysis examined the average topic half-life—how long political themes remain active in media coverage—across Fox News, ABC News, and MSNBC. Initially, all estimated half-life values were included, regardless of magnitude. However, due to the nature of exponential decay modeling, this led to highly inflated values for topics with very flat decay curves, particularly among recent or ongoing themes.\nBefore Clipping: • Average half-life estimates ranged from ~12,700 to over 20,000 days (35–55 years). • These values reflected very flat or non-decreasing topic frequencies, where the model inferred an extremely slow decay rate (λ ≈ 0). • However, such durations are not realistic in the context of news cycles, and likely reflect modeling limitations rather than true editorial behavior.\nAfter Clipping (Capped at 1000 Days): • The average topic half-life was reduced to ~926 days for Fox News, 931 days for ABC News, and 960 days for MSNBC. • Differences between sources narrowed substantially, with all three clustered within a 35-day range.\nInterpretation: • Before clipping, results were dominated by outliers and model artifacts. • After clipping, the results reflect a more realistic view of typical media attention spans, showing that topic persistence is fairly consistent across major networks when extreme cases are excluded. ## Half-Life over Time\n\n\nCode\ndef plot_yearly_half_life_trends(half_life_df: pd.DataFrame):\n    \"\"\"Plot average half-life over time by news source.\"\"\"\n    # Ensure year column exists\n    half_life_df[\"year\"] = half_life_df[\"first_date\"].dt.year\n    \n    # Group and average half-life per source per year\n    yearly = (\n        half_life_df.groupby([\"source\", \"year\"])[\"half_life_days\"]\n        .mean()\n        .reset_index()\n    )\n    \n    plt.figure(figsize=(10, 6))\n    ax = sns.lineplot(\n        data=yearly,\n        x=\"year\",\n        y=\"half_life_days\",\n        hue=\"source\",\n        marker=\"o\",\n        palette=NETWORK_COLORS, \n        errorbar=None\n    )\n    \n    plt.title(\"Average Topic Half-Life by Year and Source (Clipped at 1000 days)\")\n    plt.xlabel(\"Year of Topic Emergence\")\n    plt.ylabel(\"Average Half-Life (days)\")\n    plt.grid(True)\n    plt.legend(title=\"News Source\")\n    plt.tight_layout()\n    plt.show()\n\nplot_yearly_half_life_trends(half_life_df)\n\n\n\n\n\n\n\n\n\nThis figure shows the average topic half-life by year of emergence, clipped at 1000 days to exclude extreme values from flat or ongoing topics. From 2015 to 2020, topic half-lives remain consistently high across networks. After 2021, there is a visible decline in persistence, particularly in 2023–2025, likely reflecting the limited time window for more recent topics to exhibit decay."
  },
  {
    "objectID": "page-5.html#statistical-analysis",
    "href": "page-5.html#statistical-analysis",
    "title": "Topic Half-Life Modeling",
    "section": "2.2 Statistical Analysis",
    "text": "2.2 Statistical Analysis\nTo assess whether the distribution of topic half-lives differs significantly between news sources, we conducted a Kruskal-Wallis H-test — a non-parametric alternative to one-way ANOVA. This test is appropriate because it does not assume normality or equal variances, making it well-suited for comparing skewed distributions like topic half-lives.\nThe Kruskal-Wallis test evaluates whether samples from three or more independent groups (in this case, news sources) originate from the same distribution. The null hypothesis assumes all groups have the same median half-life; the alternative hypothesis suggests that at least one group differs. • Test statistic (H): 197.42 • p-value: 1.35 × 10⁻⁴³\nThe result is highly statistically significant (p &lt; 0.0001), strongly rejecting the null hypothesis. This indicates that at least one news source has a substantially different distribution of topic half-lives compared to the others.\nThis result suggests that the duration of media attention given to themes differs meaningfully across outlets like MSNBC, ABC News, and Fox News. For example, one network may sustain attention on political themes for significantly longer periods, while another cycles through topics more rapidly.\nWe also ran the Kruskal-Wallis test on a clipped dataset (capping topic half-life at 1000 days) to assess whether long-lived outliers were influencing results. In this filtered version, the test was not significant (H = 0.857, p = 0.355), suggesting that while extreme topic persistence differs by source, short- to medium-term attention cycles may be more comparable across networks.\n\n\n\nTable 1: Statistical significance of half-life between networks - clipped\n\n\n\nKruskal-Wallis H-test (Clipped): H=35.130, p=2.353e-08\n\n\n\n\n\n\nCode\nsns.boxplot(data=half_life_df, x=\"source\", y=\"half_life_days\", palette=NETWORK_COLORS)\nplt.title(\"Distribution of Topic Half-Lives by Source\")\nplt.ylabel(\"Half-Life (days)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot shows the distribution of individual topic half-lives by news source, with values capped at 1000 days to exclude outliers. While topics vary in duration, many reach the clipping threshold, suggesting flat or slowly decaying coverage patterns. Overall, the distributions appear visually similar across networks, supporting earlier statistical results indicating no significant difference in topic persistence by source."
  },
  {
    "objectID": "page-5.html#key-findings",
    "href": "page-5.html#key-findings",
    "title": "Topic Half-Life Modeling",
    "section": "3.1 Key Findings",
    "text": "3.1 Key Findings\n•   Initial differences in average topic half-life were inflated by outliers (topics with flat decay curves lasting decades).\n•   After capping half-lives at 1000 days, all three news sources showed very similar average attention spans.\n•   Statistical tests confirmed no significant difference in the distribution of half-lives between sources in the clipped dataset.\n•   Visual distribution and trend plots support these findings: all three networks exhibit similar patterns over time.\n•   These results suggest that, in terms of short- to medium-term coverage, mainstream news outlets tend to sustain political topics for similar durations.\n•   However, differences in long-term or ideologically anchored narratives may still exist and require more targeted investigation."
  },
  {
    "objectID": "scripts/tone_avg_prep.html",
    "href": "scripts/tone_avg_prep.html",
    "title": "Media Coverage Analysis (2015-2025)",
    "section": "",
    "text": "import pandas as pd\nimport json\n\n# Load the monthly tone averages\ntry:\n    tone_data = pd.read_csv(\"../data/gdelt_monthly_tone_averages.csv\")\n    print(f\"Loaded tone data with {len(tone_data)} records\")\nexcept FileNotFoundError:\n    print(\"Tone data file not found. Please run the tone analysis script first.\")\n    exit(1)\n\n# Calculate the average tone across all sources for each month/year\nmonthly_avg = tone_data.groupby(['year', 'month'])['average_tone'].mean().reset_index()\nprint(f\"Calculated {len(monthly_avg)} monthly averages across all sources\")\n\n# Find the min and max tone values for scaling\nmin_tone = monthly_avg['average_tone'].min()\nmax_tone = monthly_avg['average_tone'].max()\nprint(f\"Tone range: {min_tone:.2f} to {max_tone:.2f}\")\n\n# Create a nested dictionary structure for easy access in JavaScript\ntone_by_year_month = {}\n\nfor _, row in monthly_avg.iterrows():\n    year = int(row['year'])\n    month = int(row['month'])\n    tone = float(row['average_tone'])\n    \n    # Create year entry if it doesn't exist\n    if year not in tone_by_year_month:\n        tone_by_year_month[year] = {}\n    \n    # Add month entry\n    tone_by_year_month[year][month] = tone\n\n# Also create individual source dictionaries for reference\nsource_tone_data = {}\n\nfor source in tone_data['source'].unique():\n    source_data = tone_data[tone_data['source'] == source]\n    source_dict = {}\n    \n    for _, row in source_data.iterrows():\n        year = int(row['year'])\n        month = int(row['month'])\n        tone = float(row['average_tone'])\n        \n        if year not in source_dict:\n            source_dict[year] = {}\n        \n        source_dict[year][month] = tone\n    \n    source_tone_data[source] = source_dict\n\n# Create a combined data structure\ncalendar_data = {\n    'avg_tone': tone_by_year_month,\n    'source_tone': source_tone_data,\n    'metadata': {\n        'min_tone': min_tone,\n        'max_tone': max_tone\n    }\n}\n\n# Save as JavaScript file that can be included directly\nwith open(\"../data/tone_data.js\", \"w\") as f:\n    f.write(\"// Generated tone data for calendar visualization\\n\")\n    f.write(\"const toneData = \")\n    f.write(json.dumps(calendar_data, indent=2))\n    f.write(\";\\n\")\n\nprint(\"Tone data saved to website/scripts/tone_data.js\")\n\n# Also save as JSON file for reference\nwith open(\"../data/tone_data.json\", \"w\") as f:\n    json.dump(calendar_data, f, indent=2)\n\nprint(\"Tone data also saved to website/data/tone_data.json\")\n\nLoaded tone data with 369 records\nCalculated 123 monthly averages across all sources\nTone range: -3.53 to -1.93\nTone data saved to website/scripts/tone_data.js\nTone data also saved to website/data/tone_data.json"
  },
  {
    "objectID": "scripts/topics_trump.html",
    "href": "scripts/topics_trump.html",
    "title": "Media Coverage Analysis (2015-2025)",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\n# Load the CSV (replace with your actual file path)\ndata = pd.read_csv(\"../../data/topic_modeling/all_sources_headline_topics_sklearn.csv\")\n\n# Create a new column indicating if 'trump' is in the topic name (case insensitive)\ndata['has_trump'] = data['topic'].str.lower().str.contains('trump')\n\n# Filter to only include topics ranked 1-3\ntop3_data = data[data['rank'] &lt;= 3]\n\n# Group by source and calculate the percentage of top 3 topics that mention Trump\ntrump_analysis = top3_data.groupby('source').agg(\n    total_top3_topics=('topic', 'count'),\n    trump_topics=('has_trump', 'sum')\n).reset_index()\n\n# Calculate percentage\ntrump_analysis['trump_percentage'] = (trump_analysis['trump_topics'] / \n                                     trump_analysis['total_top3_topics'] * 100).round(2)\n\n# Sort by percentage (descending)\ntrump_analysis = trump_analysis.sort_values('trump_percentage', ascending=False)\n\n# Rename columns for better readability\ntrump_analysis = trump_analysis.rename(columns={\n    'source': 'Source',\n    'trump_topics': '# topics mentioning Trump',\n    'trump_percentage': '% topics mentioning Trump'\n})\n\n# Remove 'total_top3_topics' column\ntrump_analysis = trump_analysis[['Source', '# topics mentioning Trump', '% topics mentioning Trump']]\n\n# Uppercase the source names\ntrump_analysis['Source'] = trump_analysis['Source'].str.upper()\n\n# Generate a complete HTML table directly\nhtml = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;style&gt;\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #ffffff;\n    margin: 20px;\n}\ntable {\n    border-collapse: collapse;\n    width: 100%;\n    max-width: 800px;\n    margin: 0 auto;\n    border: 1px solid #500000;\n}\nth {\n    background-color: #6e0000;\n    color: white;\n    padding: 12px;\n    text-align: center;\n    font-weight: bold;\n    border: 1px solid #500000;\n}\ntd {\n    padding: 10px;\n    text-align: center;\n    border: 1px solid #500000;\n    background-color: #6e0000;\n    color: white;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;table&gt;\n    &lt;tr&gt;\n        &lt;th&gt;Source&lt;/th&gt;\n        &lt;th&gt;# topics mentioning Trump&lt;/th&gt;\n        &lt;th&gt;% topics mentioning Trump&lt;/th&gt;\n    &lt;/tr&gt;\n\"\"\"\n\n# Add rows with proper styling\nfor _, row in trump_analysis.iterrows():\n    source = row['Source']\n    topics = row['# topics mentioning Trump']\n    percentage = row['% topics mentioning Trump']\n    \n    # Determine color for source\n    if source == 'MSNBC':\n        source_color = '#6666ff'  # Blue\n    elif source == 'FOX':\n        source_color = '#ff4444'  # Red\n    elif source == 'ABC':\n        source_color = '#cc44cc'  # Purple\n    else:\n        source_color = 'white'\n    \n    # Determine background color intensity based on percentage\n    # Higher percentage = darker red\n    normalized = percentage / 100\n    r = int(110 + (180 - 110) * (1 - normalized))\n    g = int(0 + 30 * (1 - normalized))\n    b = int(0 + 30 * (1 - normalized))\n    bg_color = f'rgb({r}, {g}, {b})'\n    \n    html += f\"\"\"\n    &lt;tr&gt;\n        &lt;td style=\"color: {source_color}; font-weight: bold;\"&gt;{source}&lt;/td&gt;\n        &lt;td&gt;{topics}&lt;/td&gt;\n        &lt;td style=\"background-color: {bg_color};\"&gt;{percentage}&lt;/td&gt;\n    &lt;/tr&gt;\n    \"\"\"\n\nhtml += \"\"\"\n&lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n# Write to file\nwith open('trump_coverage_table.html', 'w') as f:\n    f.write(html)\n\nprint(\"Table created and saved as 'trump_coverage_table.html'\")\n\n# For Jupyter notebook display\ndisplay(HTML(html))\n\nTable created and saved as 'trump_coverage_table.html'\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n# topics mentioning Trump\n% topics mentioning Trump\n\n\nMSNBC\n261\n71.31\n\n\nFOX\n104\n31.23\n\n\nABC\n88\n23.85"
  }
]